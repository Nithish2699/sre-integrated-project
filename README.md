<<<<<<< HEAD

# Integrated SRE Platform Repository

This repository contains two clearly separated SRE implementations:

1. foundation-sre-platform  
   â†’ Core SRE foundations (infra, Kubernetes, observability, incidents)

2. advanced-sre-platform-argo  
   â†’ Advanced SRE practices (canary deployments, Argo Rollouts, CI/CD, error budgets)



ðŸš€ End-to-End SRE Platform: From Foundations to Advanced Reliability

This repository documents my complete SRE implementation journey, showing how I evolved from operating Kubernetes systems to enforcing reliability during deployments using canary releases, error budgets, and automation.

The project is intentionally split into two phases, each representing a clear maturity step in Site Reliability Engineering.

Repository Navigation (Click to Jump)
â€¢	Phase 1 â€” Foundation SRE Platform
   ðŸ‘‰ foundation-sre-platform/
â€¢	Phase 2 â€” Advanced SRE Platform (Argo Rollouts)
   ðŸ‘‰ advanced-sre-platform-argo/
â€¢	Incident Postmortem Example
   ðŸ‘‰ foundation-sre-platform/postmortems/real-postmortem.md
â€¢	Error Budget Policy
   ðŸ‘‰ advanced-sre-platform-argo/policy/error-budget-policy.md


Project Overview

Goal:

Build a production-grade SRE platform that demonstrates:
â€¢	Infrastructure ownership
â€¢	Kubernetes reliability operations
â€¢	Observability and incident response
â€¢	Safe, automated deployments using SLOs and error budgets
Why two phases?
Because real SRE maturity is progressive.
You must first run systems reliably before you can automate change safely.

ðŸŸ¢ PHASE 1 â€” FOUNDATION SRE PLATFORM
ðŸ“ Folder:
ðŸ‘‰ foundation-sre-platform/

ðŸŽ¯ Phase 1 Objective

Establish core SRE fundamentals required to operate production systems reliably:
â€¢	Provision infrastructure
â€¢	Deploy services
â€¢	Monitor health
â€¢	Handle incidents
â€¢	Learn from failures
This phase focuses on stability before speed.

ðŸ”¹ Step 1 â€” Provision Infrastructure (Terraform)

What:
Create a reproducible Kubernetes cluster using Infrastructure as Code.

Why:
Manual infrastructure does not scale and is error-prone.

Commands: 
cd foundation-sre-platform or cd foundation-sre-platform/terraform
terraform init
terraform apply

ðŸ”¹ Step 2 â€” Configure Cluster Access
What:
Configure kubectl access to manage workloads.

Why:
Secure access is required to deploy and operate services.

Command: 
kubectl config set-cluster sre-cluster -- 
gcloud container clusters get-credentials sre-cluster --region us-central1

ðŸ”¹ Step 3 â€” Deploy Application (Kubernetes)

What:
Deploy a containerized application with health checks.

Why:
Health probes enable self-healing and safe restarts.

Command: 

kubectl apply -f foundation-sre-platform/kubernetes/

ðŸ”¹ Step 4 â€” Install Observability (Prometheus & Grafana)

What:
Install metrics collection and dashboards.

Why:
You cannot operate what you cannot observe.

Command: 

helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update
helm install monitoring prometheus-community/kube-prometheus-stack

ðŸ”¹ Step 5 â€” Observe Metrics & Logs

What:
View latency, errors, traffic, and resource usage.

Why:
Metrics and logs are required for debugging and incident detection.

Command:

kubectl port-forward svc/monitoring-grafana 3000:80
kubectl logs <pod-name>

ðŸ”¹ Step 6 â€” Simulate Incident

What:
Inject CPU stress into a running pod.

Why:
Controlled failure validates autoscaling and monitoring.

Command:

kubectl exec -it <pod-name> -- stress-ng --cpu 2 --timeout 60s

ðŸ”¹ Step 7 â€” Blameless Postmortem

What:
Document what happened, why it happened, and how to prevent it.

Why:
SRE improves systems, not people.

ðŸ‘‰ Example:
real-postmortem.md

WHY MOVE TO PHASE 2?

After Phase 1, the system was stable and observable.

However, a critical question remained:

How do we release changes without breaking reliability?

Manual deployments and blind rollouts increase risk as systems grow.

This is where Phase 2 begins.

PHASE 2 â€” ADVANCED SRE PLATFORM (ARGOCD)

ðŸ“ Folder:
ðŸ‘‰ advanced-sre-platform-argo/ 

ðŸŽ¯ Phase 2 Objective

Enforce safe, automated deployments using:

â€¢	Canary releases
â€¢	Progressive traffic shifting
â€¢	SLO-based error-budget gating
â€¢	CI/CD automation

This phase focuses on governing change, not just deploying it.

ðŸ”¹ Step 1 â€” Install Argo Rollouts

What:
Install the controller responsible for progressive delivery.

Why:
Argo Rollouts replaces risky full rollouts with controlled canaries.

Command: 
kubectl create namespace argo-rollouts
kubectl apply -n argo-rollouts \
  -f https://github.com/argoproj/argo-rollouts/releases/latest/download/install.yaml

ðŸ”¹ Step 2 â€” Deploy Rollout (Canary Strategy)

What:
Define a rollout that shifts traffic gradually.

Why:
Small exposure reduces blast radius during failures.

Command:
kubectl apply -f advanced-sre-platform-argo/argo-rollouts/

ðŸ”¹ Step 3 â€” Traffic Shifting

What:
Send 20% â†’ 50% â†’ 100% traffic to the new version.

Why:
Gradual rollout detects issues early.

(No command â€” controlled by Argo Rollouts)

ðŸ”¹ Step 4 â€” SLO Burn-Rate Analysis

What:
Evaluate Prometheus metrics during rollout.

Why:
Deployments should stop when error budgets are at risk.

(Defined in analysis-template.yaml)

ðŸ”¹ Step 5 â€” CI/CD Trigger

What:
CI pipeline updates rollout image automatically.

Why:
CI triggers deployments, not reliability decisions.

Command:

kubectl set image rollout/sre-demo-rollout app=nginx:latest

ðŸ”¹ Step 6 â€” Automatic Rollback or Promotion

What:
Rollout is promoted or aborted automatically.

Why:
Reliability must not depend on human reaction time.

(Handled by Argo Rollouts + SLO analysis)

ðŸ”¹ Step 7 â€” Error Budget Policy

What:
Define rules that control deployment behavior.

Why:
Error budgets align reliability with business priorities.

ðŸ‘‰ Policy:
error-budget-policy.md

=======
# sre-integrated-project
>>>>>>> b708fff3922d938d6d6422b0084ae26eed34a1d5
